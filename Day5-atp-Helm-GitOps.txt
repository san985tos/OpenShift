Day 5

##### Helm CLI  ################################
##### Helm CLI  ################################
##### Helm CLI  ################################


Deploy a new project:
╰─○ oc new-project helm

Check HELM version:
╰─○ helm version

You can search for Helm Charts available in any public repositories through:
╰─○ Helm Hub.
╰─○ helm search hub nginx

By default the list of available repositories is empty. You can add a new one with the CLI. For NGINX, add Bitnami repository:
╰─○ helm repo list
╰─○ helm repo add bitnami https://charts.bitnami.com/bitnami
╰─○ helm repo list

You can search for Helm Charts also inside repos, like in the one you just installed:
╰─○ helm search repo bitnami/nginx

Deploy a Helm Chart:
╰─○ helm install my-nginx bitnami/nginx --set service.type=ClusterIP

Check your Helm releases:
╰─○ helm ls

Verify all the pods are in Running state and Ready:
╰─○ oc get pods

Now expose my-nginx service to access it via OpenShift Route:
╰─○ oc expose svc/my-nginx

Verify that route has been created:
╰─○ oc get routes

Uninstall and clean:
╰─○ helm uninstall my-nginx
╰─○ oc delete route my-nginx










##### Create your a Helm Chart  ################################
##### Create your a Helm Chart  ################################
##### Create your a Helm Chart  ################################

An Helm chart called my-chart has been already generated with the following command:
╰─○ helm create my-chart

Add the following lines on the values.yaml file
image

    ' # TODO: image repository
      repository: bitnami/nginx
      # TODO: image tag
      tag: latest
      pullPolicy: IfNotPresent
    '


Install our custom Helm Chart from local folder:
╰─○ helm install my-chart ./my-chart
╰─○ oc get pods
╰─○ helm ls

Create routes.yaml under template dir:

apiVersion: route.openshift.io/v1
kind: Route
metadata:
name: {{ include "my-chart.fullname" . }}
labels:
    {{- include "my-chart.labels" . | nindent 4 }}
spec:
port:
    targetPort: http
to:
    kind: Service
    name: {{ include "my-chart.fullname" . }}
    weight: 100
wildcardPolicy: None

Run helm upgrade to publish a new revision containing a my-charm Route:
╰─○ helm upgrade my-chart ./my-chart

Verify new Route from Terminal:
╰─○ oc get routes

Get current Revision:
╰─○ helm ls

Rollback to starting revision:
╰─○ helm rollback my-chart 1

Verify new Route from Terminal:
╰─○ oc get routes

Cleanup:
╰─○ helm uninstall my-chart
╰─○ oc delete project helm 












##### Helm integration with GitOps  ################################
##### Helm integration with GitOps  ################################
##### Helm integration with GitOps  ################################

# In this scenario, we will explore how to deploy a Helm chart using the native integration in Argo CD.

Intall OpenShift GitOps Operator from Operator Hub:
    

Let's find the URL for the ArgoCD API Server:
╰─○ oc get routes -n openshift-gitops | grep openshif

Find the default password for the admin account as follows:
╰─○ oc extract secret/openshift-gitops-cluster -n openshift-gitops --to=-

To login from the terminal, execute the following (replace $ARGOCD_SERVER_URL with the above URL):
╰─○ argocd login $ARGOCD_SERVER_URL

Apply the Argo CD Application manifest to get this Helm chart deployed:
╰─○ oc apply -f https://raw.githubusercontent.com/redhat-developer-demos/openshift-gitops-examples/main/components/applications/quarkus-app.yaml

Wait till application is done and open the route of the application:
╰─○ oc get route/quarkus-app -n demo  -o jsonpath='{.spec.host}{"\n"}'

First, check to see how many pods you have running:
╰─○ oc get pods -n demo

Now, modfy the Helm values:
╰─○ argocd app set quarkus-app -p deploy.replicas=2
╰─○ oc get pods -n demo










##### Helm GitOps Deployment  ################################
##### Helm GitOps Deployment  ################################
##### Helm GitOps Deployment  ################################


Clone this repository under the /opt directory:
╰─○ cd /opt
╰─○ git clone https://github.com/redhat-developer-demos/openshift-gitops-examples
╰─○ cd openshift-gitops-examples

Let's examine the subchart:
╰─○ tree apps/quarkus-subchart

This Chart.yaml file, is creating an "empty" Helm chart and adding the Helm chart you want to deploy as a dependency in the dependecies secion.

Here, values.yaml you're specifying the values you want to pass to the Helm chart.

The Argo CD Application should look like a "normal" git application:
╰─○ cat components/applications/quarkus-subchart.yaml

In this snippet, you can see that now we're targeting the git repo instead of the Helm repo.

Apply the Argo CD Application manifest to get this Helm chart deployed:
╰─○ oc apply -f https://raw.githubusercontent.com/redhat-developer-demos/openshift-gitops-examples/main/components/applications/quarkus-subchart.yaml

NOTE The Helm logo ⎈ does not appear. It's now a git logo since we're now just loading YAML

That's it! You've now deployed a Helm chart in a GitOps friendly way using Argo CD!

There is one important thing to note. We've deployed this Helm chart twice, and you can see this using the argocd CLI:
╰─○ argocd app list -o name

You can also see this on the cluster itself:
╰─○ oc get applications -n openshift-gitops

However, when you try and list the applications using the Helm CLI; you don't see it:
╰─○ helm ls --all-namespaces














##### Red Hat AMQ Streams  ################################
##### Red Hat AMQ Streams  ################################
##### Red Hat AMQ Streams  ################################


Create a new (project) namespace called kafka for the AMQ Streams Kafka Cluster Operator run the command:
╰─○ oc new-project kafka

Install "Red Hat Integration - AMQ Streams" operator form operator hub:


Watch the status of the pods run the following command:
╰─○ oc -n kafka get pods -w


Create a new Kafka cluster named my-cluster, with 1 ZooKeeper, 1 broker node, and ephemeral storage to simplify the deployment. In production case scenarios, you will want to increase the amount of nodes and use persistent storage. Check the configuration for the Kafka custom resource from Visual Editor or from here:
╰─○ cat /opt/kafka-cluster.yaml


[root@crc-dzk9v-master-0 /]# cat /opt/kafka-cluster.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    config:
      num.partitions: 1
      default.replication.factor: 1
      offsets.topic.replication.factor: 1
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
    storage:
      type: ephemeral
  zookeeper:
    replicas: 1
    storage:
      type: ephemeral
  entityOperator:
    topicOperator: {}

Then create the Kafka cluster by applying the configuration:
╰─○ oc -n kafka apply -f /opt/kafka-cluster.yaml
╰─○ oc -n kafka get pods -w



Check the configuration for the KafkaTopic custom resource:
╰─○ cat /opt/kafka-topic.yaml

[root@crc-dzk9v-master-0 /]# cat /opt/kafka-topic.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: my-topic
  labels:
    strimzi.io/cluster: "my-cluster"
spec:
  partitions: 1
  replicas: 1


Then apply the custom resource for the Topic Operator to pick up:
╰─○ oc -n kafka apply -f /opt/kafka-topic.yaml


Accessing the Kafka cluster from a console. AMQ streams provides container images with the Apache Kafka distribution, including console scripts. Let's connect to the running broker to execute those scripts. Support for remote container command execution is built into the oc CLI. Bash is one of the commands we can execute. To get an interactive terminal issue the following command:
╰─○ oc exec -it my-cluster-kafka-0 -- bash


Let's use it to describe the my-topic we asked the Topic Operator to create for us in the previous step. The my-topic topic has 1 partition and a replication factor of 1 as defined in the KafkaTopic resouce.:
╰─○ bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe


Run the console producer to send one message per line:
╰─○ bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-topic

You will get a prompt to start sending the messages, try with hello world!. Enter some more messages, then press Ctrl+C to stop the process:
╰─○ > hello world!


Consuming events:
╰─○ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning













##### Application messaging with OpenShift  ################################
##### Application messaging with OpenShift  ################################
##### Application messaging with OpenShift  ################################





For this scenario lets create a project called messaging by running the command:

oc new-project messaging


Install AMQ broker operator








##### ACM ################################
##### ACM ################################
##### ACM ################################

Review the installation of the Advance cluster Management topicOperator:
╰─○ Operators > Installed Operators
╰─○ Workloads > pods

Open ACM UI:
╰─○ Networking > Routes > multicloud-console 

Explore the UI.

Create AWS credentials:
╰─○ Cloud Provider: Choose Amazon Web Services
╰─○ Credential Name:  aws
╰─○ Namespace: open-cluster-management
╰─○ Base DNS Domain:  This is in the email from the RHPDS system
╰─○ Access Key ID:  This is in the email from the RHPDS system
╰─○ Secret Access Key ID: This is in the email from the RHPDS system
╰─○ Red Hat OpenShift pull secret: Get from your Red Hat login
╰─○ SSH private and public keys:  Use an existing key pair or generate a new one (see link below)

Create a new cluster in AWS
╰─○ Got to: Infrastructure > CLusters > Create cluster
  ╰─○ Infrastructure provider
    ╰─○ Select AWS
  ╰─○ Cluster details
    ╰─○ Select respective credentials 
    ╰─○ Cluster Name: ocp41
    ╰─○ Release image: 4.8
    ╰─○ Additional labels: environment=prod
  ╰─○ Node pools
    ╰─○ Region: us-west-1
  ╰─○ Click next till step 7 and click create

Create a new Single node cluster in AWS

Creating an application 
╰─○ Applications
  ╰─○ Create application
  ╰─○ Name: book-import
  ╰─○ Namespace: book-import
  ╰─○ URL: https://github.com/hichammourad/book-import.git
  ╰─○ Branch: master-no-pre-post
  ╰─○ Path: book-import
  ╰─○ SelectDeploy application resources only on clusters matching specified labels
    ╰─○ Label: environmentValue: dev
  ╰─○ Save

Edit it and change the label toenvironment=prod